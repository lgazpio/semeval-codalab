<div class="bhm-postcontent">
  <div><strong><a href="https://docs.google.com/forms/d/e/1FAIpQLScXnt7qeioCPyxu6dv9wrSDYaF04bRgVBFCUbahxsAG6F43Sg/viewform" target="_blank">Register To Participate in STS 2017</a></strong></div>
  <div><strong><a href="https://groups.google.com/forum/#!forum/sts-semeval" target="_blank">Discussion group</a></strong></div>
  <p>Semantic Textual Similarity (STS) measures the degree of equivalence in the underlying semantics of paired snippets of text. While making such an assessment is trivial for humans, constructing algorithms and computational models that mimic human level performance represents a difficult and deep natural language understanding (NLU) problem.</p>
  <p>To stimulate research in this area and encourage the development of creative new approaches to modeling sentence level semantics, the STS shared task has been held annually since 2012, as part of the SemEval/*SEM family of workshops. Each year the competition brings together numerous participating teams, diverse approaches, and ongoing improvements to state-of-the-art methods.</p>
  <p><strong>Task Definition</strong></p>
  <p>Given two sentences, participating systems are asked to return a continuous valued similarity score on a scale from 0 to 5, with 0 indicating that the semantics of the sentences are completely independent and 5 signifying semantic equivalence. Performance is assessed by computing the Pearson correlation between machine assigned semantic similarity scores and human judgements.</p>
  <p><a href="http://ixa2.si.ehu.es/stswiki/index.php/Main_Page" target="_blank"><strong><strong>Explore the STS wiki</strong></strong></a></p>
  <p><strong>Organizers (alpha. order)</strong></p>
  <p>Eneko Agirre, Daniel Cer, Mona Diab and Lucia Specia</p>
  <p>Copyright 2017 -&nbsp;<strong>SemEval-2017 Task 1. All Right Reserved</strong></p>
</div>
